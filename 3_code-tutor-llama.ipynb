{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Issues With Ollama\n",
    "This Jupyter Notebook creates a GUI interface for interacting with local Ollama AI models, providing real-time streaming responses for technical questions about programming, software engineering, cybersecurity, and LLMs. The interface features model selection dropdowns, question input areas, and displays responses as properly formatted markdown with true streaming output that updates incrementally as the AI generates content. The notebook includes robust error handling, status indicators, and a technical tutoring system that connects to the Ollama API running on localhost:11434.\n",
    "* **Initialization and Setup**\n",
    "  * Imports necessary libraries for HTTP requests, JSON handling, and Jupyter notebook widgets\n",
    "  * Sets up the Ollama API base URL (localhost:11434) and defines a system prompt for technical tutoring\n",
    "  * Creates the main GUI class that will handle all user interactions\n",
    "* **Model Discovery and Loading**\n",
    "  * Connects to the local Ollama service to fetch all available AI models\n",
    "  * Makes an API call to /api/tags endpoint to retrieve installed models\n",
    "  * Handles connection errors if Ollama isn't running or models aren't available\n",
    "  * Populates a dropdown menu with the discovered models for user selection\n",
    "* **Question Processing and Validation**\n",
    "  * Validates user input to ensure a question has been entered\n",
    "  * Checks that a valid model has been selected from the dropdown\n",
    "  * Prepares the API request with both the system prompt and user question\n",
    "  * Disables the submit button during processing to prevent multiple simultaneous requests\n",
    "* **Real-Time Streaming Response**\n",
    "  * Sends a streaming request to Ollama's chat API with the selected mode\n",
    "  * Processes incoming response chunks in real-time as they arrive from the AI\n",
    "  * Accumulates the response content and continuously updates the display\n",
    "  * Renders the response as properly formatted markdown with syntax highlighting\n",
    "  * Adds small delays between updates to create a smooth streaming visual effect\n",
    "* **Error Handling and Status Management**\n",
    "  * Monitors for various error conditions like connection failures or API errors\n",
    "  * Displays informative error messages in the output area when problems occur\n",
    "  * Updates the status indicator with color-coded messages (blue for info, green for success, red for errors)\n",
    "  * Always re-enables the submit button after processing completes, regardless of success or failure\n",
    "* **Interactive Features**\n",
    "  * Provides a refresh button to reload available models without restarting the interface\n",
    "  * Includes a clear button to remove previous responses for new questions\n",
    "  * Maintains a clean, organized layout with proper spacing and visual hierarchy\n",
    "  * Displays the complete interface in the Jupyter notebook for immediate use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1f6d071940496a9f76a230984370d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>🦙 Ollama Technical Tutor - True Markdown Streaming</h2><p>Ask technical questio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ollama Technical Tutor with True Streaming Markdown Output\n",
    "# This notebook creates a GUI interface for interacting with local Ollama models\n",
    "# providing real-time streaming responses for technical questions\n",
    "\n",
    "# Standard library imports\n",
    "import os          # For environment variable access (if needed in future)\n",
    "import requests    # For making HTTP requests to Ollama API\n",
    "import json        # For parsing JSON responses from Ollama\n",
    "import time        # For adding delays in streaming output\n",
    "\n",
    "# Jupyter notebook specific imports\n",
    "import ipywidgets as widgets                              # For creating interactive GUI widgets\n",
    "from IPython.display import display, clear_output, Markdown  # For displaying content and markdown\n",
    "\n",
    "# Ollama API configuration\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"  # Default Ollama server URL\n",
    "\n",
    "# System prompt for technical tutoring - defines the AI's role and behavior\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful technical tutor who answers questions about programming, software engineering, cybersecurity and LLMs. \n",
    "Provide clear, detailed explanations with examples when appropriate. Break down complex concepts into understandable parts \n",
    "and offer practical guidance for implementation.\"\"\"\n",
    "\n",
    "class OllamaTutorMarkdownStreamingGUI:\n",
    "    \"\"\"\n",
    "    A GUI class for creating an interactive technical tutor using Ollama models.\n",
    "    \n",
    "    This class provides a complete interface for:\n",
    "    - Selecting available Ollama models\n",
    "    - Inputting technical questions\n",
    "    - Streaming markdown responses in real-time\n",
    "    - Managing conversation sessions\n",
    "    \n",
    "    Attributes:\n",
    "        available_models (list): List of available Ollama models\n",
    "        current_output: Placeholder for output management\n",
    "        model_dropdown: Widget for model selection\n",
    "        question_input: Widget for question input\n",
    "        submit_button: Widget for submitting questions\n",
    "        clear_button: Widget for clearing responses\n",
    "        output_area: Widget for displaying streaming responses\n",
    "        status_label: Widget for showing status messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the GUI interface.\n",
    "        \n",
    "        Sets up the complete interface by:\n",
    "        1. Loading available models from Ollama\n",
    "        2. Creating all GUI widgets\n",
    "        3. Arranging widgets in the layout\n",
    "        \"\"\"\n",
    "        self.available_models = []\n",
    "        self.current_output = None\n",
    "        self.load_available_models()\n",
    "        self.setup_widgets()\n",
    "        self.setup_layout()\n",
    "    \n",
    "    def load_available_models(self):\n",
    "        \"\"\"\n",
    "        Fetch available models from the local Ollama installation.\n",
    "        \n",
    "        Makes an API call to Ollama's /api/tags endpoint to retrieve\n",
    "        all installed models. Handles connection errors gracefully\n",
    "        and provides informative error messages.\n",
    "        \n",
    "        Updates:\n",
    "            self.available_models: List of model names or error messages\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Make GET request to Ollama API to fetch available models\n",
    "            response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "            if response.status_code == 200:\n",
    "                models_data = response.json()\n",
    "                # Extract model names from the API response\n",
    "                self.available_models = [model['name'] for model in models_data.get('models', [])]\n",
    "                if not self.available_models:\n",
    "                    self.available_models = [\"No models found - Please install models first\"]\n",
    "            else:\n",
    "                self.available_models = [\"Error connecting to Ollama\"]\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            # Handle case where Ollama service is not running\n",
    "            self.available_models = [\"Ollama not running - Please start Ollama service\"]\n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected errors\n",
    "            self.available_models = [f\"Error: {str(e)}\"]\n",
    "    \n",
    "    def setup_widgets(self):\n",
    "        \"\"\"\n",
    "        Create and configure all GUI widgets for the interface.\n",
    "        \n",
    "        Creates:\n",
    "        - Model selection dropdown\n",
    "        - Refresh button for updating model list\n",
    "        - Text area for question input\n",
    "        - Submit and clear buttons\n",
    "        - Output area for streaming responses\n",
    "        - Status label for user feedback\n",
    "        \n",
    "        Also binds event handlers to interactive widgets.\n",
    "        \"\"\"\n",
    "        # Model selection dropdown - allows user to choose from available models\n",
    "        self.model_dropdown = widgets.Dropdown(\n",
    "            options=self.available_models,\n",
    "            value=self.available_models[0] if self.available_models else \"No models available\",\n",
    "            description='Model:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # Refresh button - updates the model list without restarting\n",
    "        self.refresh_button = widgets.Button(\n",
    "            description='Refresh Models',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        # Question input area - large text field for technical questions\n",
    "        self.question_input = widgets.Textarea(\n",
    "            placeholder='Enter your technical question here...',\n",
    "            description='Question:',\n",
    "            layout=widgets.Layout(width='600px', height='100px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Submit button - processes the question and streams response\n",
    "        self.submit_button = widgets.Button(\n",
    "            description='Ask Question',\n",
    "            button_style='success',  # Green color for primary action\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        # Clear button - removes current response for new questions\n",
    "        self.clear_button = widgets.Button(\n",
    "            description='Clear Response',\n",
    "            button_style='warning',  # Orange color for secondary action\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        # Output area - displays streaming markdown responses\n",
    "        self.output_area = widgets.Output()\n",
    "        \n",
    "        # Status indicator - shows current operation status with colored messages\n",
    "        self.status_label = widgets.HTML(\n",
    "            value=\"<b>Status:</b> Ready to answer your question\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "        \n",
    "        # Bind event handlers to interactive widgets\n",
    "        self.submit_button.on_click(self.on_submit_clicked)\n",
    "        self.clear_button.on_click(self.on_clear_clicked)\n",
    "        self.refresh_button.on_click(self.on_refresh_clicked)\n",
    "    \n",
    "    def setup_layout(self):\n",
    "        \"\"\"\n",
    "        Arrange all widgets into a cohesive interface layout.\n",
    "        \n",
    "        Creates a vertical layout with:\n",
    "        - Header with title and description\n",
    "        - Model selection row with refresh button\n",
    "        - Question input area\n",
    "        - Control buttons row\n",
    "        - Status indicator\n",
    "        - Response output area\n",
    "        \"\"\"\n",
    "        # Header section with title and description\n",
    "        header = widgets.HTML(\n",
    "            value=\"<h2>🦙 Ollama Technical Tutor - True Markdown Streaming</h2><p>Ask technical questions with real-time markdown streaming output</p>\",\n",
    "            layout=widgets.Layout(margin='0px 0px 20px 0px')\n",
    "        )\n",
    "        \n",
    "        # Model selection row - dropdown and refresh button side by side\n",
    "        model_row = widgets.HBox([\n",
    "            self.model_dropdown,\n",
    "            widgets.HTML(value=\"&nbsp;\" * 3),  # Spacer for visual separation\n",
    "            self.refresh_button\n",
    "        ], layout=widgets.Layout(margin='10px 0px'))\n",
    "        \n",
    "        # Control buttons row - submit and clear buttons side by side\n",
    "        controls_row = widgets.HBox([\n",
    "            self.submit_button,\n",
    "            widgets.HTML(value=\"&nbsp;\" * 5),  # Spacer for visual separation\n",
    "            self.clear_button\n",
    "        ], layout=widgets.Layout(margin='10px 0px'))\n",
    "        \n",
    "        # Main interface - vertical arrangement of all components\n",
    "        self.interface = widgets.VBox([\n",
    "            header,\n",
    "            model_row,\n",
    "            self.question_input,\n",
    "            controls_row,\n",
    "            self.status_label,\n",
    "            widgets.HTML(value=\"<b>Response:</b>\"),\n",
    "            self.output_area\n",
    "        ], layout=widgets.Layout(padding='20px'))\n",
    "    \n",
    "    def on_submit_clicked(self, button):\n",
    "        \"\"\"\n",
    "        Handle submit button click events.\n",
    "        \n",
    "        Validates user input and initiates the streaming response process.\n",
    "        Performs input validation and model selection verification before\n",
    "        calling the streaming response method.\n",
    "        \n",
    "        Args:\n",
    "            button: The button widget that triggered the event (unused)\n",
    "        \"\"\"\n",
    "        # Get and validate user question input\n",
    "        question = self.question_input.value.strip()\n",
    "        \n",
    "        if not question:\n",
    "            self.update_status(\"Please enter a question before submitting.\", \"error\")\n",
    "            return\n",
    "        \n",
    "        # Validate model selection\n",
    "        selected_model = self.model_dropdown.value\n",
    "        if \"Error\" in selected_model or \"No models\" in selected_model:\n",
    "            self.update_status(\"Please select a valid model or refresh the model list.\", \"error\")\n",
    "            return\n",
    "        \n",
    "        # Initiate streaming response\n",
    "        self.stream_markdown_response(question)\n",
    "    \n",
    "    def on_clear_clicked(self, button):\n",
    "        \"\"\"\n",
    "        Handle clear button click events.\n",
    "        \n",
    "        Clears the output area and updates status to indicate readiness\n",
    "        for new questions.\n",
    "        \n",
    "        Args:\n",
    "            button: The button widget that triggered the event (unused)\n",
    "        \"\"\"\n",
    "        # Clear the output display area\n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "        self.update_status(\"Response cleared. Ready for a new question.\", \"success\")\n",
    "    \n",
    "    def on_refresh_clicked(self, button):\n",
    "        \"\"\"\n",
    "        Handle refresh button click events.\n",
    "        \n",
    "        Reloads the list of available models from Ollama and updates\n",
    "        the dropdown widget with the current model list.\n",
    "        \n",
    "        Args:\n",
    "            button: The button widget that triggered the event (unused)\n",
    "        \"\"\"\n",
    "        self.update_status(\"Refreshing available models...\", \"info\")\n",
    "        # Reload models from Ollama API\n",
    "        self.load_available_models()\n",
    "        # Update dropdown options with new model list\n",
    "        self.model_dropdown.options = self.available_models\n",
    "        if self.available_models:\n",
    "            self.model_dropdown.value = self.available_models[0]\n",
    "        self.update_status(\"Models refreshed successfully!\", \"success\")\n",
    "    \n",
    "    def update_status(self, message, status_type=\"info\"):\n",
    "        \"\"\"\n",
    "        Update the status label with colored messages.\n",
    "        \n",
    "        Provides visual feedback to users about the current state of operations\n",
    "        using color-coded messages for different types of status updates.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The status message to display\n",
    "            status_type (str): Type of status - 'info', 'success', 'error', or 'warning'\n",
    "        \"\"\"\n",
    "        # Color mapping for different status types\n",
    "        colors = {\n",
    "            \"info\": \"#2196F3\",      # Blue for informational messages\n",
    "            \"success\": \"#4CAF50\",   # Green for successful operations\n",
    "            \"error\": \"#f44336\",     # Red for error conditions\n",
    "            \"warning\": \"#FF9800\"    # Orange for warnings\n",
    "        }\n",
    "        \n",
    "        color = colors.get(status_type, colors[\"info\"])\n",
    "        self.status_label.value = f\"<b style='color: {color}'>Status:</b> {message}\"\n",
    "    \n",
    "    def stream_markdown_response(self, question):\n",
    "        \"\"\"\n",
    "        Stream the AI response as markdown in real-time.\n",
    "        \n",
    "        This is the core method that:\n",
    "        1. Sends the question to Ollama API with streaming enabled\n",
    "        2. Processes the streaming response chunks\n",
    "        3. Updates the display with accumulated markdown content\n",
    "        4. Handles errors gracefully with informative messages\n",
    "        \n",
    "        Args:\n",
    "            question (str): The user's technical question to be answered\n",
    "        \"\"\"\n",
    "        selected_model = self.model_dropdown.value\n",
    "        \n",
    "        # Update UI state for processing\n",
    "        self.update_status(f\"Streaming response from {selected_model}...\", \"info\")\n",
    "        self.submit_button.disabled = True  # Prevent multiple simultaneous requests\n",
    "        \n",
    "        # Clear previous response from output area\n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "        \n",
    "        # Prepare API request payload with system prompt and user question\n",
    "        payload = {\n",
    "            \"model\": selected_model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            \"stream\": True  # Enable streaming response\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make streaming POST request to Ollama chat API\n",
    "            response = requests.post(\n",
    "                f\"{OLLAMA_BASE_URL}/api/chat\",\n",
    "                json=payload,\n",
    "                stream=True,\n",
    "                timeout=60  # 60 second timeout for long responses\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                accumulated_response = \"\"  # Build complete response incrementally\n",
    "                \n",
    "                # Process each line of the streaming response\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        try:\n",
    "                            # Parse JSON chunk from streaming response\n",
    "                            chunk_data = json.loads(line.decode('utf-8'))\n",
    "                            \n",
    "                            # Extract content from the message chunk\n",
    "                            if 'message' in chunk_data and 'content' in chunk_data['message']:\n",
    "                                chunk_content = chunk_data['message']['content']\n",
    "                                accumulated_response += chunk_content\n",
    "                                \n",
    "                                # Update display with current accumulated response\n",
    "                                with self.output_area:\n",
    "                                    clear_output(wait=True)  # Clear previous content\n",
    "                                    display(Markdown(accumulated_response))  # Display as markdown\n",
    "                                \n",
    "                                # Small delay for smooth visual streaming effect\n",
    "                                time.sleep(0.03)\n",
    "                            \n",
    "                            # Check if streaming is complete\n",
    "                            if chunk_data.get('done', False):\n",
    "                                break\n",
    "                                \n",
    "                        except json.JSONDecodeError:\n",
    "                            # Skip malformed JSON chunks and continue\n",
    "                            continue\n",
    "                \n",
    "                # Update status when streaming completes successfully\n",
    "                self.update_status(\"Response completed successfully!\", \"success\")\n",
    "            else:\n",
    "                # Handle HTTP error responses from Ollama API\n",
    "                error_msg = f\"Ollama API error: {response.status_code}\"\n",
    "                self.update_status(error_msg, \"error\")\n",
    "                \n",
    "                with self.output_area:\n",
    "                    clear_output()\n",
    "                    display(Markdown(f\"**❌ Error:** {error_msg}\\n\\nPlease check if Ollama is running and the model is available.\"))\n",
    "                \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            # Handle connection errors when Ollama service is not available\n",
    "            error_msg = \"Cannot connect to Ollama. Please ensure Ollama is running on localhost:11434\"\n",
    "            self.update_status(error_msg, \"error\")\n",
    "            \n",
    "            with self.output_area:\n",
    "                clear_output()\n",
    "                display(Markdown(f\"**❌ Connection Error:** {error_msg}\"))\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected errors during streaming\n",
    "            error_msg = f\"Unexpected error: {str(e)}\"\n",
    "            self.update_status(error_msg, \"error\")\n",
    "            \n",
    "            with self.output_area:\n",
    "                clear_output()\n",
    "                display(Markdown(f\"**❌ Error:** {error_msg}\"))\n",
    "        \n",
    "        finally:\n",
    "            # Always re-enable the submit button when processing completes\n",
    "            self.submit_button.disabled = False\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"\n",
    "        Display the complete GUI interface in the Jupyter notebook.\n",
    "        \n",
    "        This method renders the entire interface, making it visible and\n",
    "        interactive for users. Should be called after instantiating the class.\n",
    "        \"\"\"\n",
    "        display(self.interface)\n",
    "\n",
    "# Create and display the Ollama technical tutor GUI\n",
    "# This instantiates the class and immediately displays the interface\n",
    "ollama_tutor = OllamaTutorMarkdownStreamingGUI()\n",
    "ollama_tutor.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
